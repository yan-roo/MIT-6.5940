# TinyML & Efficient Deep Learning Computing â€” Labs (MITÂ 6.5940,â€¯Fallâ€¯2024)

Implementation notes and code for the **five lab assignments** of MITÂ 6.5940 â€” *TinyML and Efficient Deep Learning Computing*.

---

## ğŸ“šÂ Course Snapshot

| Item          | Value                                                                    |
| ------------- | ------------------------------------------------------------------------ |
| Lecturer      | Prof.Â SongÂ Han (MITÂ HANÂ Lab)                                             |
| Semester      | **FallÂ 2024**                                                            |
| Units / Level | 3â€‘0â€‘9, Graduate (PhDâ€‘level)                                              |
| Grading       | 5Â LabsÂ Ã—Â 15â€¯%Â +Â FinalÂ ProjectÂ 25â€¯%Â (+ proposal / report / participation) |
| Prerequisites | 6.1910Â (ComputationÂ Structures) & 6.3900Â (IntroÂ toÂ ML)                   |

---

## ğŸ—‚Â Repository Layout

```text
.
â”œâ”€â”€ lab1.ipynb           # fineâ€‘grained & channel pruning
â”œâ”€â”€ lab2.ipynb      # Wâ€‘bit / Aâ€‘bit postâ€‘training quant
â”œâ”€â”€ lab3.ipynb               # differentiable NAS & hardwareâ€‘aware search
â”œâ”€â”€ lab4.ipynb   # sparse / quant LLM, KV cache sharing
â”œâ”€â”€ lab5.ipynb    # run Llamaâ€‘2â€‘7B on laptop CPU/GPU
â””â”€â”€ README.md
```

---

## ğŸ“Â Lab Overview

| Lab   | Theme                      | Release â†’ Due      | Core skills                                              |
| ----- | -------------------------- | ------------------ | -------------------------------------------------------- |
| **1** | PruningÂ &Â Sparsity         | 2024â€‘09â€‘17Â â†’Â 09â€‘26 | fineâ€‘grained / channel pruning, FLOPsÂ & latency analysis |
| **2** | Quantization               | 2024â€‘09â€‘26Â â†’Â 10â€‘08 | PTQ / QAT, dynamicâ€‘range calibration,Â INT8 speedâ€‘up      |
| **3** | Neural Architecture Search | 2024â€‘10â€‘08Â â†’Â 10â€‘22 | differentiable NAS, HW cost models, Pareto frontier      |
| **4** | LLMÂ Compression            | 2024â€‘10â€‘22Â â†’Â 10â€‘31 | sparseÂ +Â quantÂ LLM, MoE gating, retrieval cache          |
| **5** | LLMÂ DeploymentÂ onÂ Laptop   | 2024â€‘10â€‘31Â â†’Â 11â€‘12 | ggml / vLLM runtime, CPU offloading, memory mapping      |
